{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4d78a8",
   "metadata": {},
   "source": [
    "# Lab 04: Matrix Vision -- Lab Notebook\n",
    "**Image Processing & Steganography with NumPy Arrays**\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the Digital Forensics Image Analysis Division. You have been assigned the role of **Computer Vision Analyst** on a high-priority case. A set of surveillance images has been intercepted, and intelligence suggests they may contain hidden data encoded using steganography -- the practice of concealing messages within ordinary-looking images.\n",
    "\n",
    "Your mission is threefold:\n",
    "\n",
    "1. **Understand** how digital images are represented as numerical arrays.\n",
    "2. **Build** a custom image processing toolkit using only raw NumPy operations -- no high-level image processing libraries.\n",
    "3. **Decode** a hidden message embedded in the least significant bits of pixel values.\n",
    "\n",
    "**Constraints:** You may use Pillow (`PIL`) only for loading and saving images. You may use Matplotlib only for visualization. All image processing must be implemented with raw NumPy array operations. No OpenCV, scikit-image, or scipy.\n",
    "\n",
    "**Reference:** Consult [`concepts.md`](concepts.md) for detailed background on NumPy arrays, image representation, convolution theory, and bitwise operations.\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 1: Field Work (Understanding Images as Arrays)\n",
    "\n",
    "Before we can analyze surveillance footage, we need to understand how a computer represents an image. At its core, every digital image is a grid of numbers -- a matrix. A color image is a 3D array of shape `(height, width, 3)`, where each pixel holds three values: Red, Green, and Blue, each ranging from 0 to 255.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1.1: Loading an Image as a NumPy Array\n",
    "\n",
    "Our first task is to load a surveillance image into memory and inspect its structure. We use Pillow only to read the file from disk, then immediately convert it to a NumPy array for all subsequent work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the primary surveillance image\n",
    "img = np.array(Image.open('data/surveillance_a.png'))\n",
    "\n",
    "# Inspect the array structure\n",
    "print(f\"Shape: {img.shape}\")           # (height, width, channels)\n",
    "print(f\"Dtype: {img.dtype}\")           # Data type of each element\n",
    "print(f\"Min: {img.min()}, Max: {img.max()}\")  # Value range\n",
    "print(f\"Pixel at (0,0): {img[0, 0]}\")  # First pixel [R, G, B]\n",
    "print(f\"Total pixels: {img.shape[0] * img.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0207c448",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "Shape: (300, 300, 3)\n",
    "Dtype: uint8\n",
    "Min: 0, Max: 255\n",
    "Pixel at (0,0): [40 40 60]\n",
    "Total pixels: 90000\n",
    "```\n",
    "\n",
    "The shape tells us the image is 300 pixels tall, 300 pixels wide, and has 3 color channels. The dtype `uint8` means unsigned 8-bit integers (range 0-255). The pixel at position (0,0) is the top-left corner.\n",
    "</details>\n",
    "\n",
    "**Task:** Now load `surveillance_b.png` and record its properties. How does it compare to `surveillance_a.png`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5780ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load surveillance_b.png and inspect its properties\n",
    "img_b = np.array(Image.open('data/surveillance_b.png'))\n",
    "print(f\"Shape: {img_b.shape}\")\n",
    "print(f\"Dtype: {img_b.dtype}\")\n",
    "print(f\"Min: {img_b.min()}, Max: {img_b.max()}\")\n",
    "print(f\"Pixel at (0,0): {img_b[0, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeeef72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 1.2: Visualizing with Matplotlib\n",
    "\n",
    "Numbers alone do not tell the whole story. We need to see what the surveillance images actually look like. Matplotlib's `imshow()` function can render a NumPy array as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Surveillance A\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img_b)\n",
    "axes[1].set_title(\"Surveillance B\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ace649",
   "metadata": {},
   "source": [
    "**Key pattern to remember:** This `plt.subplots()` / `imshow()` / `axis('off')` / `tight_layout()` / `show()` pattern is the standard way to display images throughout this lab. You will use it repeatedly.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1.3: Channel Isolation\n",
    "\n",
    "A color image has three layers stacked on top of each other: Red, Green, and Blue. We can extract each channel independently using array slicing on the third dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract individual channels\n",
    "red_channel   = img[:, :, 0]  # All rows, all columns, channel 0 (Red)\n",
    "green_channel = img[:, :, 1]  # All rows, all columns, channel 1 (Green)\n",
    "blue_channel  = img[:, :, 2]  # All rows, all columns, channel 2 (Blue)\n",
    "\n",
    "print(f\"Red channel shape: {red_channel.shape}\")    # (300, 300) -- 2D!\n",
    "print(f\"Green channel shape: {green_channel.shape}\")\n",
    "print(f\"Blue channel shape: {blue_channel.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize each channel as a grayscale image\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(red_channel, cmap='gray')\n",
    "axes[0].set_title(\"Red Channel\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(green_channel, cmap='gray')\n",
    "axes[1].set_title(\"Green Channel\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(blue_channel, cmap='gray')\n",
    "axes[2].set_title(\"Blue Channel\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3299c3eb",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "Red channel shape: (300, 300)\n",
    "Green channel shape: (300, 300)\n",
    "Blue channel shape: (300, 300)\n",
    "```\n",
    "\n",
    "Each channel is a 2D array. When displayed with `cmap='gray'`, bright areas indicate high values in that channel and dark areas indicate low values. For example, the red square in `surveillance_a.png` should appear bright in the Red channel and dark in the Green and Blue channels.\n",
    "</details>\n",
    "\n",
    "**Question:** A single channel is a 2D array with values 0-255. When displayed, why does it appear as grayscale rather than colored? What is Matplotlib actually doing when you pass `cmap='gray'`?\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 2: The Build (Manual Image Filters)\n",
    "\n",
    "Now that we understand how images are structured as arrays, it is time to build our forensic image processing toolkit. Each filter you implement will be a function in `image_filters.py`. You will write the function, then test it here in the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "### Part A: Grayscale Conversion (15 mins)\n",
    "\n",
    "**Objective:** Convert a color image to grayscale using the luminosity method.\n",
    "\n",
    "**The Science:** The human eye does not perceive all colors equally. We are most sensitive to green light, moderately sensitive to red, and least sensitive to blue. The standard luminosity formula accounts for this:\n",
    "\n",
    "```\n",
    "gray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
    "```\n",
    "\n",
    "This produces a perceptually accurate grayscale image -- much better than a simple average of the three channels.\n",
    "\n",
    "**Task:** Implement the `to_grayscale()` function in `image_filters.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61596711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(img):\n",
    "    \"\"\"Convert RGB image to grayscale using luminosity method.\n",
    "\n",
    "    Args:\n",
    "        img: NumPy array of shape (H, W, 3), dtype uint8\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of shape (H, W), dtype uint8\n",
    "    \"\"\"\n",
    "    # Hint: Use array slicing for each channel, apply weights,\n",
    "    #       sum them, and convert back to uint8.\n",
    "    # Remember: multiplying a uint8 array by a float automatically\n",
    "    #           promotes to float64 -- but you must cast back at the end.\n",
    "    # TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5694c2",
   "metadata": {},
   "source": [
    "**Test your implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_filters import to_grayscale\n",
    "\n",
    "gray = to_grayscale(img)\n",
    "print(f\"Input shape:  {img.shape}\")    # (300, 300, 3)\n",
    "print(f\"Output shape: {gray.shape}\")   # (300, 300)\n",
    "print(f\"Output dtype: {gray.dtype}\")   # uint8\n",
    "print(f\"Output range: {gray.min()} - {gray.max()}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Original (Color)\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(gray, cmap='gray')\n",
    "axes[1].set_title(\"Grayscale (Luminosity)\")\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3ee4d",
   "metadata": {},
   "source": [
    "**Verification:** Compare your result against the Pillow built-in conversion and the provided reference image. Your values should match within +/-1 (rounding differences are acceptable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31935d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare against PIL's grayscale conversion\n",
    "pil_gray = np.array(Image.open('data/surveillance_a.png').convert('L'))\n",
    "difference = np.abs(gray.astype(int) - pil_gray.astype(int))\n",
    "print(f\"Max difference from PIL: {difference.max()}\")\n",
    "print(f\"Mean difference from PIL: {difference.mean():.4f}\")\n",
    "\n",
    "# Compare against the provided reference\n",
    "ref_gray = np.array(Image.open('data/reference/grayscale_ref.png'))\n",
    "exact_match = np.array_equal(gray, ref_gray)\n",
    "print(f\"Exact match with reference: {exact_match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7747686",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "Max difference from PIL: 1\n",
    "Mean difference from PIL: 0.2xxx\n",
    "Exact match with reference: True\n",
    "```\n",
    "\n",
    "A max difference of 1 from PIL is normal -- it is caused by different rounding strategies. Your output should be an exact match with the reference image since it uses the same formula.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Part B: Color Inversion (10 mins)\n",
    "\n",
    "**Objective:** Create a photographic negative by computing the complement of each pixel value.\n",
    "\n",
    "**The Math:** For any pixel value in the range [0, 255], the inversion is simply:\n",
    "\n",
    "```\n",
    "inverted = 255 - original\n",
    "```\n",
    "\n",
    "Black (0) becomes white (255). White (255) becomes black (0). Red (255, 0, 0) becomes cyan (0, 255, 255). Every color maps to its complement.\n",
    "\n",
    "**Task:** Implement the `invert()` function in `image_filters.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9891318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(img):\n",
    "    \"\"\"Invert all pixel values (create a negative).\n",
    "\n",
    "    Args:\n",
    "        img: NumPy array of shape (H, W, 3) or (H, W), dtype uint8\n",
    "\n",
    "    Returns:\n",
    "        NumPy array with same shape, dtype uint8\n",
    "    \"\"\"\n",
    "    # Hint: This is a single NumPy operation thanks to broadcasting.\n",
    "    # NumPy will apply the subtraction to every element in the array.\n",
    "    # TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b2d65",
   "metadata": {},
   "source": [
    "**Test your implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_filters import invert\n",
    "\n",
    "inv = invert(img)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(inv)\n",
    "axes[1].set_title(\"Inverted (Negative)\")\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verification: inverting twice should return the original\n",
    "double_inv = invert(inv)\n",
    "print(f\"Double inversion matches original: {np.array_equal(double_inv, img)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd5a83",
   "metadata": {},
   "source": [
    "**Question:** The `invert()` function works on both color images (shape `(H, W, 3)`) and grayscale images (shape `(H, W)`) without any code change. Why? What NumPy feature makes this possible?\n",
    "\n",
    "---\n",
    "\n",
    "### Part C: Brightness & Contrast Adjustment (15 mins)\n",
    "\n",
    "**Objective:** Adjust image brightness and contrast using a linear transformation.\n",
    "\n",
    "**The Math:** Each pixel is transformed by:\n",
    "\n",
    "```\n",
    "adjusted = clip(alpha * pixel + beta, 0, 255)\n",
    "```\n",
    "\n",
    "- `alpha` controls **contrast**: 1.0 = no change, >1.0 = more contrast, <1.0 = less contrast.\n",
    "- `beta` controls **brightness**: 0 = no change, >0 = brighter, <0 = darker.\n",
    "- `clip` ensures values stay within the valid [0, 255] range.\n",
    "\n",
    "**CRITICAL WARNING -- uint8 Overflow:**\n",
    "\n",
    "Before implementing, you must understand a dangerous property of `uint8` arithmetic. Run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: uint8 overflow wraps around!\n",
    "a = np.uint8(200)\n",
    "b = np.uint8(100)\n",
    "print(f\"200 + 100 = {a + b}\")   # Expected: 300. Actual: 44!\n",
    "print(f\"  Why? 300 - 256 = 44 (wraps around)\")\n",
    "\n",
    "# This WILL corrupt your images if you don't convert to float first!\n",
    "c = np.uint8(10)\n",
    "d = np.uint8(50)\n",
    "print(f\"10 - 50 = {c - d}\")     # Expected: -40. Actual: 216!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20c231",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "200 + 100 = 44\n",
    "  Why? 300 - 256 = 44 (wraps around)\n",
    "10 - 50 = 216\n",
    "```\n",
    "\n",
    "The `uint8` type can only hold values 0-255. Arithmetic that exceeds this range silently wraps around, producing incorrect results. The fix: always convert to `float64` before doing arithmetic, then clip and cast back to `uint8`.\n",
    "</details>\n",
    "\n",
    "**Task:** Implement `adjust_brightness_contrast()` in `image_filters.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beea54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_brightness_contrast(img, alpha=1.0, beta=0):\n",
    "    \"\"\"Adjust brightness and contrast.\n",
    "\n",
    "    Args:\n",
    "        img: NumPy array, dtype uint8\n",
    "        alpha: Contrast multiplier (1.0 = no change, >1 = more contrast)\n",
    "        beta: Brightness offset (0 = no change, >0 = brighter)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array with same shape, dtype uint8\n",
    "    \"\"\"\n",
    "    # Step 1: Convert to float64 to avoid uint8 overflow\n",
    "    # Step 2: Apply the formula: alpha * pixel + beta\n",
    "    # Step 3: Clip values to [0, 255]\n",
    "    # Step 4: Convert back to uint8\n",
    "    # TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35464b4a",
   "metadata": {},
   "source": [
    "**Test your implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf50f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_filters import adjust_brightness_contrast\n",
    "\n",
    "bright = adjust_brightness_contrast(img, alpha=1.4, beta=30)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(bright)\n",
    "axes[1].set_title(\"Brightness +30, Contrast x1.4\")\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify no overflow occurred\n",
    "print(f\"Output dtype: {bright.dtype}\")\n",
    "print(f\"Output range: {bright.min()} - {bright.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81861f51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Part D: Thresholding (15 mins)\n",
    "\n",
    "**Objective:** Convert a grayscale image into a pure black-and-white (binary) image using a threshold value.\n",
    "\n",
    "**The Concept:** Thresholding is one of the simplest forms of image segmentation. Every pixel above the threshold becomes white (255), and every pixel at or below becomes black (0). This separates the \"foreground\" from the \"background.\"\n",
    "\n",
    "```\n",
    "binary[y, x] = 255 if gray[y, x] > threshold else 0\n",
    "```\n",
    "\n",
    "**Task:** Implement `threshold()` in `image_filters.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(gray_img, thresh=128):\n",
    "    \"\"\"Apply binary thresholding to a grayscale image.\n",
    "\n",
    "    Args:\n",
    "        gray_img: NumPy array of shape (H, W), dtype uint8\n",
    "        thresh: Threshold value (0-255)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of shape (H, W), dtype uint8, values are 0 or 255\n",
    "    \"\"\"\n",
    "    # Hint: A boolean comparison like (gray_img > thresh) produces a\n",
    "    # True/False array. Multiplying by 255 converts True to 255 and\n",
    "    # False to 0. Don't forget to cast to uint8!\n",
    "    # TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331eb9a",
   "metadata": {},
   "source": [
    "**Test your implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_filters import threshold\n",
    "\n",
    "# First, convert to grayscale, then threshold\n",
    "gray = to_grayscale(img)\n",
    "binary = threshold(gray, thresh=128)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(gray, cmap='gray')\n",
    "axes[0].set_title(\"Grayscale\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(binary, cmap='gray')\n",
    "axes[1].set_title(\"Threshold (128)\")\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify output contains only 0 and 255\n",
    "unique_values = np.unique(binary)\n",
    "print(f\"Unique values in output: {unique_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a4491",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "Unique values in output: [  0 255]\n",
    "```\n",
    "\n",
    "The binary image should contain exactly two values: 0 (black) and 255 (white). If you see other values, check your implementation.\n",
    "</details>\n",
    "\n",
    "**Experiment:** Try different threshold values (64, 128, 192) and observe how the segmentation changes. What threshold best separates the shapes from the background in `surveillance_a.png`?\n",
    "\n",
    "---\n",
    "\n",
    "### Part E: Box Blur -- Convolution (25 mins)\n",
    "\n",
    "**Objective:** Implement a box blur filter using manual convolution with nested loops.\n",
    "\n",
    "**The Concept:** A box blur replaces each pixel with the average of its neighborhood. For a 3x3 kernel, that means averaging the pixel and its 8 neighbors:\n",
    "\n",
    "```\n",
    "kernel = [[1/9, 1/9, 1/9],\n",
    "          [1/9, 1/9, 1/9],\n",
    "          [1/9, 1/9, 1/9]]\n",
    "```\n",
    "\n",
    "For a detailed explanation of how convolution works step-by-step, including why kernels must be odd-sized and how to handle edge pixels, see the **Convolution** section in [`concepts.md`](concepts.md).\n",
    "\n",
    "**Task:** Implement `box_blur()` in `image_filters.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e34aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_blur(img, kernel_size=3):\n",
    "    \"\"\"Apply box blur using manual convolution.\n",
    "\n",
    "    Args:\n",
    "        img: NumPy array of shape (H, W) or (H, W, 3), dtype uint8\n",
    "        kernel_size: Size of the square kernel (must be odd)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array with same shape, dtype uint8\n",
    "    \"\"\"\n",
    "    assert kernel_size % 2 == 1, \"Kernel size must be odd\"\n",
    "    pad = kernel_size // 2\n",
    "\n",
    "    # For color images, blur each channel independently\n",
    "    if img.ndim == 3:\n",
    "        return np.stack([box_blur(img[:, :, c], kernel_size)\n",
    "                         for c in range(img.shape[2])], axis=-1)\n",
    "\n",
    "    # Pad edges with reflected values to handle border pixels\n",
    "    padded = np.pad(img.astype(np.float64), pad, mode='reflect')\n",
    "    h, w = img.shape\n",
    "    result = np.zeros((h, w), dtype=np.float64)\n",
    "\n",
    "    # Slide the kernel over every pixel\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            # Extract the neighborhood around pixel (y, x)\n",
    "            neighborhood = padded[y:y + kernel_size, x:x + kernel_size]\n",
    "            # The new pixel value is the mean of the neighborhood\n",
    "            result[y, x] = neighborhood.mean()\n",
    "\n",
    "    return np.clip(result, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca9c21",
   "metadata": {},
   "source": [
    "**Note:** The nested-loop approach above is intentionally written for clarity, not speed. A 300x300 image with a 5x5 kernel requires 300 * 300 * 25 = 2,250,000 operations. Real image processing libraries use optimized C code or FFT-based convolution for performance. For this lab, correctness matters more than speed.\n",
    "\n",
    "**Test your implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34433814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_filters import box_blur\n",
    "\n",
    "blurred_3 = box_blur(img, kernel_size=3)\n",
    "blurred_5 = box_blur(img, kernel_size=5)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(blurred_3)\n",
    "axes[1].set_title(\"Box Blur (3x3)\")\n",
    "axes[1].axis('off')\n",
    "axes[2].imshow(blurred_5)\n",
    "axes[2].set_title(\"Box Blur (5x5)\")\n",
    "axes[2].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original - sharp edges visible\")\n",
    "print(f\"3x3 blur - slightly smoothed\")\n",
    "print(f\"5x5 blur - noticeably smoothed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b125fae2",
   "metadata": {},
   "source": [
    "**Question:** What happens to edges and fine details as you increase the kernel size? Why does a larger kernel produce a stronger blur?\n",
    "\n",
    "---\n",
    "\n",
    "### Filter Showcase\n",
    "\n",
    "Now that all five filters are implemented, let us apply them all to `surveillance_a.png` and display the results in a single figure. This serves as both a visual summary and a verification that all your implementations work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_filters import to_grayscale, invert, adjust_brightness_contrast, threshold, box_blur\n",
    "\n",
    "# Load the image\n",
    "img = np.array(Image.open('data/surveillance_a.png'))\n",
    "\n",
    "# Apply all filters\n",
    "gray = to_grayscale(img)\n",
    "inv = invert(img)\n",
    "bright = adjust_brightness_contrast(img, alpha=1.4, beta=30)\n",
    "binary = threshold(gray, thresh=128)\n",
    "blurred = box_blur(img, kernel_size=5)\n",
    "\n",
    "# Display in a 2x3 grid\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle(\"Matrix Vision -- Filter Showcase\", fontsize=16)\n",
    "\n",
    "panels = [\n",
    "    (axes[0, 0], img,     \"Original\"),\n",
    "    (axes[0, 1], gray,    \"Grayscale (Luminosity)\"),\n",
    "    (axes[0, 2], inv,     \"Inverted\"),\n",
    "    (axes[1, 0], bright,  \"Brightness +30, Contrast x1.4\"),\n",
    "    (axes[1, 1], binary,  \"Threshold (128)\"),\n",
    "    (axes[1, 2], blurred, \"Box Blur (5x5)\"),\n",
    "]\n",
    "\n",
    "for ax, data, label in panels:\n",
    "    if data.ndim == 2:\n",
    "        ax.imshow(data, cmap='gray')\n",
    "    else:\n",
    "        ax.imshow(data)\n",
    "    ax.set_title(label)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291900bf",
   "metadata": {},
   "source": [
    "If all six panels render correctly, your filter toolkit is complete. Proceed to Phase 3.\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 3: Critical Incident -- Steganography\n",
    "\n",
    "**ESCALATION NOTICE:** Intelligence has confirmed that the intercepted surveillance images contain hidden data. Analysis suggests the adversary is using **Least Significant Bit (LSB) steganography** to transmit secret messages. The message is invisible to the human eye because modifying the lowest bit of a pixel value (e.g., changing 214 to 215) produces an imperceptible color change.\n",
    "\n",
    "Your task is to understand the encoding technique and decode the hidden message from `data/stego_image.png`.\n",
    "\n",
    "For detailed background on binary numbers, bitwise operations, and the steganography protocol, consult the relevant sections in [`concepts.md`](concepts.md).\n",
    "\n",
    "---\n",
    "\n",
    "### Part A: Understanding LSB Encoding (10 mins)\n",
    "\n",
    "**The Core Insight:** Every pixel value (0-255) is stored as 8 binary bits. The **Most Significant Bit** (MSB, bit 7) is worth 128 -- changing it produces a dramatic color shift. The **Least Significant Bit** (LSB, bit 0) is worth only 1 -- changing it is invisible to the eye.\n",
    "\n",
    "```\n",
    "Original pixel:  11010110  (214)\n",
    "Modified pixel:  11010111  (215)  <-- LSB changed from 0 to 1\n",
    "Difference: 1 out of 255 (0.4%) -- INVISIBLE\n",
    "```\n",
    "\n",
    "This means we can \"hide\" one bit of secret data in every pixel by writing our data into the LSB. To store a byte (8 bits) of information, we need 8 pixels.\n",
    "\n",
    "**Demonstration:** Let us extract and visualize the LSB layer of our surveillance images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both a normal image and the suspected stego image\n",
    "normal_img = np.array(Image.open('data/surveillance_a.png'))\n",
    "stego_img  = np.array(Image.open('data/stego_image.png'))\n",
    "\n",
    "# Extract the LSB of every pixel in the Red channel\n",
    "normal_lsb = (normal_img[:, :, 0] & 1) * 255  # Bitwise AND with 1, scale to visible\n",
    "stego_lsb  = (stego_img[:, :, 0] & 1) * 255\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(normal_lsb, cmap='gray')\n",
    "axes[0].set_title(\"LSB Layer: surveillance_a (Normal)\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(stego_lsb, cmap='gray')\n",
    "axes[1].set_title(\"LSB Layer: stego_image (Suspect)\")\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c96ee1",
   "metadata": {},
   "source": [
    "**Question:** Examine the two LSB layers carefully. Does the normal image's LSB layer look like random noise? What about the stego image -- can you see any patterns in the top rows where data might be encoded?\n",
    "\n",
    "---\n",
    "\n",
    "### Part B: Decoding the Hidden Message (25 mins)\n",
    "\n",
    "**Objective:** Extract the secret message from `data/stego_image.png`.\n",
    "\n",
    "**The Encoding Protocol (Declassified):**\n",
    "\n",
    "1. The message is hidden in the LSBs of the **Red channel only**.\n",
    "2. The first **32 pixels** encode the message length as a 32-bit unsigned integer (MSB first).\n",
    "3. The remaining pixels encode the message as **ASCII characters** (8 bits per character, MSB first).\n",
    "\n",
    "**Task:** Implement `extract_lsb_message()` in `steganography.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdac0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lsb_message(img):\n",
    "    \"\"\"Extract a hidden message from the LSBs of the Red channel.\n",
    "\n",
    "    Args:\n",
    "        img: NumPy array of shape (H, W, 3), dtype uint8\n",
    "\n",
    "    Returns:\n",
    "        str: The decoded hidden message\n",
    "    \"\"\"\n",
    "    # Step 1: Flatten the Red channel to a 1D array\n",
    "    red_channel = img[:, :, 0].flatten()\n",
    "\n",
    "    # Step 2: Extract LSBs using bitwise AND\n",
    "    lsbs = red_channel & 1\n",
    "\n",
    "    # Step 3: Read the first 32 bits as message length\n",
    "    # Hint: Iterate through bits, shift and accumulate:\n",
    "    #   msg_length = 0\n",
    "    #   for bit in lsbs[:32]:\n",
    "    #       msg_length = (msg_length << 1) | int(bit)\n",
    "    # TODO: Implement length extraction\n",
    "\n",
    "    # Step 4: Read the message bits (after the 32-bit header)\n",
    "    # Hint: Group bits into 8-bit chunks, convert each to ASCII:\n",
    "    #   message = ''\n",
    "    #   for i in range(msg_length):\n",
    "    #       byte_bits = lsbs[32 + i*8 : 32 + (i+1)*8]\n",
    "    #       char_value = 0\n",
    "    #       for bit in byte_bits:\n",
    "    #           char_value = (char_value << 1) | int(bit)\n",
    "    #       message += chr(char_value)\n",
    "    # TODO: Implement message extraction\n",
    "\n",
    "    # Step 5: Return the decoded message\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3ae09",
   "metadata": {},
   "source": [
    "**Test your implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75002d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from steganography import extract_lsb_message\n",
    "\n",
    "stego_img = np.array(Image.open('data/stego_image.png'))\n",
    "message = extract_lsb_message(stego_img)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  STEGANOGRAPHY DECODE RESULT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Message length: {len(message)} characters\")\n",
    "print(f\"  Decoded message: \\\"{message}\\\"\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac383e8b",
   "metadata": {},
   "source": [
    "If your implementation is correct, the decoded message will be readable English text. Record this message in your `submission.md` file.\n",
    "\n",
    "**Capacity Analysis:** How much data could this image hold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81fd4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = stego_img.shape[:2]\n",
    "total_red_pixels = h * w\n",
    "usable_bits = total_red_pixels - 32  # subtract header\n",
    "max_chars = usable_bits // 8\n",
    "print(f\"Image dimensions: {h} x {w}\")\n",
    "print(f\"Total Red channel pixels: {total_red_pixels}\")\n",
    "print(f\"Maximum message length: {max_chars} characters ({max_chars} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7f325d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Part C: Encoding a Message (Bonus)\n",
    "\n",
    "**Objective:** Write the reverse function -- hide your own message inside an image.\n",
    "\n",
    "If you can decode, you should be able to encode. Implement the `encode_lsb_message()` function in `steganography.py` that takes an image and a message string, and returns a new image with the message hidden in the Red channel LSBs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea511f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_lsb_message(img, message):\n",
    "    \"\"\"Hide a message in the LSBs of the Red channel.\n",
    "\n",
    "    Args:\n",
    "        img: NumPy array of shape (H, W, 3), dtype uint8\n",
    "        message: str to hide (ASCII only)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array (modified copy) with message encoded in LSBs\n",
    "    \"\"\"\n",
    "    # TODO: Implement the reverse of extract_lsb_message\n",
    "    # 1. Make a copy of the image (don't modify the original)\n",
    "    # 2. Convert message length to 32-bit binary (MSB first)\n",
    "    # 3. Convert each character to 8-bit binary (MSB first)\n",
    "    # 4. For each bit, clear the LSB of the target Red pixel,\n",
    "    #    then set it to the desired bit: (pixel & 0xFE) | bit\n",
    "    # 5. Return the modified image\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e14cc",
   "metadata": {},
   "source": [
    "**Test the round-trip:** Encode a message, then decode it. The result should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48af1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from steganography import encode_lsb_message, extract_lsb_message\n",
    "\n",
    "# Use a clean image as the carrier\n",
    "carrier = np.array(Image.open('data/surveillance_a.png'))\n",
    "secret = \"Testing 123 -- this message is hidden in plain sight!\"\n",
    "\n",
    "# Encode\n",
    "encoded_img = encode_lsb_message(carrier, secret)\n",
    "\n",
    "# Verify the image looks unchanged\n",
    "print(f\"Max pixel difference: {np.abs(carrier.astype(int) - encoded_img.astype(int)).max()}\")\n",
    "\n",
    "# Decode\n",
    "recovered = extract_lsb_message(encoded_img)\n",
    "print(f\"Original:  \\\"{secret}\\\"\")\n",
    "print(f\"Recovered: \\\"{recovered}\\\"\")\n",
    "print(f\"Match: {secret == recovered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49faf8",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "Max pixel difference: 1\n",
    "Original:  \"Testing 123 -- this message is hidden in plain sight!\"\n",
    "Recovered: \"Testing 123 -- this message is hidden in plain sight!\"\n",
    "Match: True\n",
    "```\n",
    "\n",
    "The maximum pixel difference should be 1 (only the LSB changes). The recovered message should be an exact match.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## Wrap-Up\n",
    "\n",
    "Congratulations, Analyst. You have successfully completed the Matrix Vision investigation.\n",
    "\n",
    "**What you accomplished today:**\n",
    "\n",
    "1. **Loaded and inspected** digital images as NumPy arrays, understanding that a color image is a 3D matrix of shape `(H, W, 3)` with `uint8` values.\n",
    "2. **Implemented five image filters** from scratch using only array math: grayscale conversion, color inversion, brightness/contrast adjustment, binary thresholding, and box blur convolution.\n",
    "3. **Discovered the uint8 overflow trap** and learned why type casting to float is essential before arithmetic operations on pixel data.\n",
    "4. **Decoded a hidden steganographic message** using bitwise operations on the least significant bits of pixel values.\n",
    "5. **Visualized** all results using Matplotlib for side-by-side comparison and verification.\n",
    "\n",
    "**Before you leave:**\n",
    "\n",
    "- Complete all sections of [`submission.md`](submission.md), including the decoded message, capacity analysis, and reflection questions.\n",
    "- Ensure your `image_filters.py`, `steganography.py`, and `main.py` files run without errors.\n",
    "- Include your AI Usage Appendix if applicable.\n",
    "\n",
    "**Looking ahead:** The array manipulation and vectorized operations you practiced today form the foundation for processing large-scale data. In upcoming labs, you will apply similar patterns to audio signals, streaming data, and more.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
